{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07e63776-cbec-415e-9c2f-9d13f24fca90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "mlflow.set_experiment(\"/Workspace/ecommerce_purchase_prediction\")\n",
    "\n",
    "os.environ[\"MLFLOW_DFS_TMP\"] = \"/Volumes/workspace/default/my_volume/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0426f2b4-6112-495d-b626-09dbd8077671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.spark.log_model(\n",
    "    model,\n",
    "    \"model\",\n",
    "    dfs_tmpdir=\"/Volumes/workspace/default/my_volume/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "573b6472-c4b8-48ca-9b42-ac4a42febdf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"total_events\",\n",
    "        \"cart_count\",\n",
    "        \"view_count\",\n",
    "        \"total_spent\",\n",
    "        \"avg_purchase_price\"\n",
    "    ],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "gold_df=spark.read.table(\"workspace.ecommerce.ml_training_gold\")\n",
    "assembled_df = assembler.transform(gold_df)\n",
    "final_df = assembled_df.select(\"features\", \"purchased\")\n",
    "\n",
    "train_data, test_data = final_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "730a7d42-b15b-48ff-935d-0760f0efb6dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT * FROM workspace.ecommerce.ml_training_gold\").printSchema()\n",
    "assembled_df.printSchema()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f906846-af23-4880-b7f4-4349495dcb45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol=\"purchased\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"Logistic_Baseline\"):\n",
    "\n",
    "    lr = LogisticRegression(\n",
    "        labelCol=\"purchased\",\n",
    "        featuresCol=\"features\"\n",
    "    )\n",
    "\n",
    "    model = lr.fit(train_data)\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "\n",
    "    mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
    "    mlflow.log_param(\"regParam\", lr.getRegParam())\n",
    "    mlflow.log_param(\"elasticNetParam\", lr.getElasticNetParam())\n",
    "    mlflow.log_metric(\"AUC\", auc)\n",
    "\n",
    "    mlflow.spark.log_model(\n",
    "        model,\n",
    "        \"model\",\n",
    "        dfs_tmpdir=\"/Volumes/workspace/default/my_volume/\"\n",
    "    )\n",
    "\n",
    "    print(\"Logged Logistic Baseline AUC:\", auc)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2be28574-0188-4cc9-b564-a350779613df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "with mlflow.start_run(run_name=\"RF_Tuned\"):\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        labelCol=\"purchased\",\n",
    "        featuresCol=\"features\",\n",
    "        numTrees=100,\n",
    "        maxDepth=10\n",
    "    )\n",
    "\n",
    "    model = rf.fit(train_data)\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "    mlflow.log_param(\"numTrees\", 100)\n",
    "    mlflow.log_param(\"maxDepth\", 10)\n",
    "\n",
    "    # Log metric\n",
    "    mlflow.log_metric(\"AUC\", auc)\n",
    "\n",
    "    # Log model\n",
    "    mlflow.spark.log_model(model, \"model\")\n",
    "\n",
    "    print(\"Logged RF Tuned AUC:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7434b377-c4a3-47c3-8cd6-cfa6c83489ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day-7",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}